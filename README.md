## üìñ √çndice

1. [Introdu√ß√£o](#introdu√ß√£o)
2. [Objetivos](#objetivos)
3. [Metodologia](#metodologia)
4. [Pontos Positivos](#pontos-positivos)
5. [Pontos Negativos](#pontos-negativos)
6. [Avalia√ß√£o CPU vs GPU](#avalia√ß√£o-cpu-vs-gpu)
7. [Resultados e Percep√ß√µes Pessoais](#resultados-e-percep√ß√µes-pessoais)
8. [Conclus√£o](#conclus√£o)

---

## Introdu√ß√£o

Este reposit√≥rio cont√©m a reprodu√ß√£o e an√°lise do tutorial oficial do **TensorFlow** para constru√ß√£o de um modelo **Transformer** aplicado √† tradu√ß√£o autom√°tica (Ingl√™s ‚Üí Portugu√™s).

O Transformer √© um dos modelos mais relevantes em **Processamento de Linguagem Natural (PLN)**, servindo de base para arquiteturas modernas como BERT, GPT e T5. Este trabalho busca n√£o apenas reproduzir o tutorial, mas tamb√©m refletir sobre seus pontos fortes, limita√ß√µes e desempenho em diferentes cen√°rios de hardware (CPU vs GPU).

---

## Objetivos

* **Reproduzir** o tutorial original de tradu√ß√£o autom√°tica utilizando Transformer.
* **Documentar** percep√ß√µes pessoais sobre a experi√™ncia de implementa√ß√£o e execu√ß√£o.
* **Comparar** o desempenho do treinamento em CPU e GPU.
* **Refletir** sobre as aplica√ß√µes pr√°ticas e os desafios de usar Transformers em cen√°rios reais.

---

## Metodologia

1. **Implementa√ß√£o**: execu√ß√£o do notebook `tutorial_transformer_revised.ipynb`, que possui 82 c√©lulas de c√≥digo e 155 c√©lulas de documenta√ß√£o em Markdown.
2. **Treinamento**: rodado em ambientes distintos ‚Äî CPU e GPU.
3. **Avalia√ß√£o**: an√°lise qualitativa (facilidade de aprendizado, clareza do c√≥digo) e quantitativa (tempo de treinamento, escalabilidade).
4. **Documenta√ß√£o**: registro de pontos positivos e negativos, al√©m da experi√™ncia pessoal durante a execu√ß√£o.

---

## Pontos Positivos

* **Clareza e progress√£o pedag√≥gica**: o tutorial conduz o leitor passo a passo, tornando conceitos complexos mais acess√≠veis.
* **Estrutura modular do c√≥digo**: fun√ß√µes e classes bem separadas facilitam experimenta√ß√£o e personaliza√ß√£o.
* **Integra√ß√£o com `tf.keras`**: uso de API de alto n√≠vel que permite callbacks, m√©tricas e otimizadores sem esfor√ßo adicional.
* **Resultados vis√≠veis**: mesmo com dataset reduzido, √© poss√≠vel observar aprendizado real nas tradu√ß√µes.
* **Exposi√ß√£o ao mecanismo de *self-attention***: fundamental para compreens√£o pr√°tica de como os Transformers funcionam.

---

## Pontos Negativos

* **Tempo de execu√ß√£o em CPU**: excessivamente elevado, inviabilizando m√∫ltiplos testes ou ajustes de hiperpar√¢metros.
* **Complexidade conceitual**: a quantidade de novos conceitos (positional encoding, m√°scaras, multi-head attention) pode sobrecarregar iniciantes.
* **Alto custo computacional**: consumo significativo de mem√≥ria, especialmente em GPUs menores, podendo gerar erros de *out of memory*.
* **Dataset limitado**: base de dados pequena n√£o reflete o verdadeiro potencial do modelo.
* **Pouca explora√ß√£o de hiperpar√¢metros**: o tutorial segue configura√ß√µes fixas, reduzindo a experimenta√ß√£o.

---

## Avalia√ß√£o CPU vs GPU

### Treinamento em CPU

* **Tempo por √©poca:** elevado, chegando a horas mesmo para datasets pequenos.
* **Produtividade:** invi√°vel para itera√ß√µes r√°pidas.
* **Utilidade:** apenas para rodar c√©lulas isoladas e entender o fluxo do c√≥digo.

### Treinamento em GPU

* **Tempo por √©poca:** reduzido de forma significativa (ordens de magnitude mais r√°pido).
* **Produtividade:** permite explorar par√¢metros, rodar m√∫ltiplas √©pocas e observar converg√™ncia da perda.
* **Utilidade:** essencial para qualquer aplica√ß√£o pr√°tica ou acad√™mica que envolva Transformers.

üìä **Resumo Comparativo**

| Aspecto            | CPU                           | GPU                              |
| ------------------ | ----------------------------- | -------------------------------- |
| Tempo de treino    | Muito alto (horas/dias)       | Baixo (minutos/horas)            |
| Produtividade      | Frustrante, pouco pr√°tica     | Fluida, permite experimenta√ß√£o   |
| Escalabilidade     | Quase invi√°vel                | Suporta datasets maiores         |
| Consumo energ√©tico | Elevado (processo prolongado) | Mais eficiente em treinos curtos |

---

## Resultados e Percep√ß√µes Pessoais

* O tutorial √© extremamente did√°tico e cumpre bem o papel de **introduzir o Transformer**.
* No entanto, percebi que **sem GPU, o aprendizado √© quase invi√°vel**: a experi√™ncia em CPU √© limitada a execu√ß√µes de teste, sem possibilidade real de explorar aprendizado profundo.
* A utiliza√ß√£o em GPU mostrou a **evolu√ß√£o real das tradu√ß√µes** ao longo das √©pocas, o que refor√ßa o valor da acelera√ß√£o por hardware.
* Pessoalmente, o maior ganho foi a **compreens√£o do self-attention**, algo que antes parecia apenas te√≥rico e abstrato.
* Apesar disso, fiquei com a sensa√ß√£o de que para **aplica√ß√µes concretas**, seria necess√°rio:

  * usar datasets maiores,
  * investir em ajustes de hiperpar√¢metros,
  * e dispor de infraestrutura mais completa.