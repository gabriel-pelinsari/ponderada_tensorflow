## üìñ √çndice

1. [Introdu√ß√£o](#introdu√ß√£o)
2. [Objetivos](#objetivos)
3. [Metodologia](#metodologia)
4. [Pontos Positivos](#pontos-positivos)
5. [Pontos Negativos](#pontos-negativos)
6. [Avalia√ß√£o CPU vs GPU](#avalia√ß√£o-cpu-vs-gpu)
7. [Resultados e Percep√ß√µes Pessoais](#resultados-e-percep√ß√µes-pessoais)
8. [Conclus√£o](#conclus√£o)

---

## Introdu√ß√£o

Este reposit√≥rio apresenta a reprodu√ß√£o e an√°lise pr√°tica de um modelo Transformer aplicado √† tarefa de tradu√ß√£o autom√°tica de frases do Portugu√™s para o Ingl√™s. O trabalho √© baseado no tutorial oficial do TensorFlow, mas com adapta√ß√µes e coment√°rios cr√≠ticos que visam destacar tanto os aspectos t√©cnicos quanto as limita√ß√µes do m√©todo.

O Transformer, introduzido no artigo cl√°ssico ‚ÄúAttention is All You Need‚Äù (Vaswani et al., 2017), revolucionou o campo de Processamento de Linguagem Natural (PLN) ao substituir arquiteturas recorrentes (RNNs, LSTMs) e convolucionais por um mecanismo de autoaten√ß√£o (self-attention). Essa inova√ß√£o permitiu ganhos expressivos em desempenho, paralelismo e capacidade de capturar depend√™ncias de longo alcance em sequ√™ncias de texto, tornando-se a base de modelos amplamente utilizados, como BERT, GPT e T5.

Neste projeto, foi implementada uma vers√£o reduzida do Transformer para fins educacionais, utilizando o dataset TED Talks (Portugu√™s ‚Üî Ingl√™s), disponibilizado pelo TensorFlow Datasets. O foco principal foi compreender:

Como o modelo √© constru√≠do a partir de camadas de embedding, codifica√ß√£o posicional, aten√ß√£o multi-cabe√ßas e feed-forward.

- Como organizar e treinar um pipeline eficiente de dados com tf.data.

- O impacto do treinamento em diferentes hardwares (CPU vs GPU).

- A import√¢ncia de t√©cnicas auxiliares, como scheduler de taxa de aprendizado e m√©tricas mascaradas, para garantir estabilidade no aprendizado.

Al√©m de reproduzir tradu√ß√µes autom√°ticas, o projeto inclui visualiza√ß√µes de mapas de aten√ß√£o, permitindo interpretar como o modelo associa palavras entre os dois idiomas, e a exporta√ß√£o do modelo treinado, viabilizando seu uso em aplica√ß√µes reais.

---

## Objetivos

* **Reproduzir** o tutorial original de tradu√ß√£o autom√°tica utilizando Transformer.
* **Documentar** percep√ß√µes pessoais sobre a experi√™ncia de implementa√ß√£o e execu√ß√£o.
* **Comparar** o desempenho do treinamento em CPU e GPU.
* **Refletir** sobre as aplica√ß√µes pr√°ticas e os desafios de usar Transformers em cen√°rios reais.

---

## Metodologia

1. **Implementa√ß√£o**: execu√ß√£o do notebook `tutorial_transformer_revised.ipynb`, que possui 82 c√©lulas de c√≥digo e 155 c√©lulas de documenta√ß√£o em Markdown.
2. **Treinamento**: rodado em ambientes distintos ‚Äî CPU e GPU.
3. **Avalia√ß√£o**: an√°lise qualitativa (facilidade de aprendizado, clareza do c√≥digo) e quantitativa (tempo de treinamento, escalabilidade).
4. **Documenta√ß√£o**: registro de pontos positivos e negativos, al√©m da experi√™ncia pessoal durante a execu√ß√£o.

---

## Pontos Positivos

* **Clareza e progress√£o pedag√≥gica**: o tutorial conduz o leitor passo a passo, tornando conceitos complexos mais acess√≠veis.
* **Estrutura modular do c√≥digo**: fun√ß√µes e classes bem separadas facilitam experimenta√ß√£o e personaliza√ß√£o.
* **Integra√ß√£o com `tf.keras`**: uso de API de alto n√≠vel que permite callbacks, m√©tricas e otimizadores sem esfor√ßo adicional.
* **Resultados vis√≠veis**: mesmo com dataset reduzido, √© poss√≠vel observar aprendizado real nas tradu√ß√µes.
* **Exposi√ß√£o ao mecanismo de *self-attention***: fundamental para compreens√£o pr√°tica de como os Transformers funcionam.

---

## Pontos Negativos

* **Tempo de execu√ß√£o em CPU**: excessivamente elevado, inviabilizando m√∫ltiplos testes ou ajustes de hiperpar√¢metros.
* **Complexidade conceitual**: a quantidade de novos conceitos (positional encoding, m√°scaras, multi-head attention) pode sobrecarregar iniciantes.
* **Alto custo computacional**: consumo significativo de mem√≥ria, especialmente em GPUs menores, podendo gerar erros de *out of memory*.
* **Dataset limitado**: base de dados pequena n√£o reflete o verdadeiro potencial do modelo.
* **Pouca explora√ß√£o de hiperpar√¢metros**: o tutorial segue configura√ß√µes fixas, reduzindo a experimenta√ß√£o.

---

## Avalia√ß√£o CPU vs GPU

### Treinamento em CPU

* **Tempo por √©poca:** elevado, chegando a horas mesmo para datasets pequenos.
* **Produtividade:** invi√°vel para itera√ß√µes r√°pidas.
* **Utilidade:** apenas para rodar c√©lulas isoladas e entender o fluxo do c√≥digo.

### Treinamento em GPU

* **Tempo por √©poca:** reduzido de forma significativa (ordens de magnitude mais r√°pido).
* **Produtividade:** permite explorar par√¢metros, rodar m√∫ltiplas √©pocas e observar converg√™ncia da perda.
* **Utilidade:** essencial para qualquer aplica√ß√£o pr√°tica ou acad√™mica que envolva Transformers.

üìä **Resumo Comparativo**

| Aspecto            | CPU                           | GPU                              |
| ------------------ | ----------------------------- | -------------------------------- |
| Tempo de treino    | Muito alto (horas/dias)       | Baixo (minutos/horas)            |
| Produtividade      | Frustrante, pouco pr√°tica     | Fluida, permite experimenta√ß√£o   |
| Escalabilidade     | Quase invi√°vel                | Suporta datasets maiores         |
| Consumo energ√©tico | Elevado (processo prolongado) | Mais eficiente em treinos curtos |

---

## Resultados e Percep√ß√µes Pessoais

* O tutorial √© extremamente did√°tico e cumpre bem o papel de **introduzir o Transformer**.
* No entanto, percebi que **sem GPU, o aprendizado √© quase invi√°vel**: a experi√™ncia em CPU √© limitada a execu√ß√µes de teste, sem possibilidade real de explorar aprendizado profundo.
* A utiliza√ß√£o em GPU mostrou a **evolu√ß√£o real das tradu√ß√µes** ao longo das √©pocas, o que refor√ßa o valor da acelera√ß√£o por hardware.
* Pessoalmente, o maior ganho foi a **compreens√£o do self-attention**, algo que antes parecia apenas te√≥rico e abstrato.
* Apesar disso, fiquei com a sensa√ß√£o de que para **aplica√ß√µes concretas**, seria necess√°rio:

  * usar datasets maiores,
  * investir em ajustes de hiperpar√¢metros,
  * e dispor de infraestrutura mais completa.